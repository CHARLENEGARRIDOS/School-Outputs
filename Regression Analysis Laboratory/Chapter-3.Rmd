---
title: "Regression Models for Data Science in R"
author: "Charlene Garridos"
date: '2023-03-08'
output: 
  pdf_document:
    toc: true
    number_sections: true
    highlight: tango
---

# Ordinary Least Squares

**Ordinary least squares (OLS)** is the *workhorse of statistics*. It gives a way of taking complicated outcomes and explaining behavior (such as trends) using linearity. The simplest application of OLS is fitting a line.

## General least squares for linear equations

<https://www.youtube.com/watch?v=LapyH7MG3Q4&list=PLpl-gQkQivXjqHAJd2t-J_One_fYE55tC&index=6>


**Fitting the best line:**
Let $Y_{i}$ be the $i^{th}$ child's height and $X_{i}$ be the $i^{th}$ (average over the pair of) parents' heights. Consider finding the best line 

>Child's Height = $\beta_{0}$ + Parent's Height $\beta_{1}$.


*Use least squares*
  $$
  \sum_{i=1}^n \{Y_{i} - (\beta_{0} + \beta_{1} X_{i})\}^2
  $$

>**Note**: Minimizing this equation will minimize the sum of the squared distances between the fitted line at the parents’ heights $\beta_{i}X_{i}$ and the observed child heights $Y_{i}$.

**Result:**
The least squares of the line:

$$
Y = \beta_{0} + \beta_{i}X_{i}  
$$

through the data pairs $X_{i}, Y_{i}$ with $Y_{i}$ as the *outcome* obtains the line $Y = \hat\beta_{0} + \hat\beta_{1}X$ where:

$$
\hat\beta_{1} = Cor(Y,X)\frac{Sd(Y)}{Sd(X)} \ and \ \hat\beta_{0} = \bar{Y} = \hat\beta_{1}\bar{X} 
$$

**Elaborate:**

• $\hat\beta_{1}$ has the units of $Y/X$, $\hat\beta_{0}$ has the units of $Y$.

• The line passes through the point $(\bar{X}, \bar{Y}$).

• The slope of the regression line with $X$ as the outcome and $Y$ as the predictor is $Cor(Y, X) Sd(X)/ Sd(Y)$.The slope is the same one you would get if you centered the data, $(X_{i} - \bar{X}, Y_{i} - \bar{Y})$, and did regression through the origin.


Regression through the origin, assuming that $\beta_{0} = 0$, yields the following solution to the *least squares criteria*:

$$
\hat\beta_{1} = \frac{\sum_{i=1}^{n}X_{i}Y_{i}}{\sum_{i=1}^{n}X_{i}^2}
$$

>**Note:** If you normalized the data, $\{ \frac{X_i - \bar X}{Sd(X)}, \frac{Y_i - \bar Y}{Sd(Y)}\}$, the slope is $Cor(Y, X)$.


## Revisisting Galton's Data

<https://www.youtube.com/watch?v=O7cDyrjWBBc&index=7&list=PLpl-gQkQivXjqHAJd2t-J_One_fYE55tC>


## Showing the OLS Result

Proof of why the ordinary least squares result works out to be the way
that it is: 

<https://www.youtube.com/watch?v=COVQX8WZVA8&index=8&list=PLpl-gQkQivXjqHAJd2t-J_One_fYE55tC>
