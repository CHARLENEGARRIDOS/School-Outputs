---
title: "Regression Models for Data Science in R"
subtitle: "Chapter - 2: Notation"
author: "Charlene Garridos"
date: '2023-03-07'
output: 
  pdf_document:
    toc: true
    number_sections: true
    highlight: tango
---

# Notations

## Notations for Data

We write $X_{1}, X_{2},\ldots,X_{n}$ to describe $n$ data points. As an example, consider the data set ${1,2,5}$ then $X_{1} = 1, X_{2} = 2, X_{3} = 5$ and $n=3$.

>**Note**: Use *Greek letters* such as, $\mu$ being a population mean that weâ€™d like to estimate.

## The Emperical Mean 

The *empirical mean* is a *measure of center of our data*. Under sampling assumptions, it estimates a population mean of interest. Define the empirical mean as
$$
\bar{X} = \frac{1}{n}\sum_{i=1}^{n} X_{i}
$$

Notice if we subtract the mean from data points, we get data that has mean 0. That is, if we define
$$
\bar{X}_{i} = X_{i}-\bar{X}.
$$

then the mean of the $\tilde{X}_{i}$ is 0. This process is called *centering the random variables*. 

*Empirical mean* is the *least squares solution for minimizing*.
$$
\sum_{i=1}^{n}(\bar{X}_{i}-\mu)^2
$$

## The Emperical Standard Deviation and Variance

The *variance and standard deviation* are *measures of how spread out our data is*. Under sampling assumptions, they estimate variability in the population. We define the empirical variance as:
$$
S^2 = \frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\bar{X})^2 = \frac{1}{n-1}(\sum_{i=1}^{n}X_{i}^2-n\bar{X}^2)
$$

The empirical standard deviation is defined as $S = \sqrt {S^2}$

Notice that the standard deviation has the same units as the data. The data defined by$Xi/s$ have empirical standard deviation 1. This is called **scaling** the data.

## Normalization

The data defined by:
$$
Z_{i} = \frac{{X}_{i}-\bar{X}}{s}
$$

has empirical mean zero and empirical standard deviation 1. The process of centering then scaling the data is called **normalizing** the data. Normalized data are centered at 0 and have units equal to standard deviations of the original data.

*Normalization* is very useful for creating data that comparable across experiments by getting rid of any shifting or scaling effects.

## The Emperical Covariance

*Empirical covariance* is defined as:
$$
Cov(X,Y) = \frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\bar{X})(Y_{i}-\bar{Y}) = \frac{1}{n-1}(\sum_{i=1}^{n}X_{i}Y_{i}-n\bar{X}\bar{Y})
$$

This measure is of limited utility, since its units are the product of the units of the two variables

The **correlation** is defined as:
$$
Cor(X,Y) = \frac{Cov(X,Y)}{S_{x}S_{y}}
$$

where $S_{x}$ and $S_{y}$ are the estimates of standard deviations for the $X$ observations and $Y$ observations,respectively. The correlation is simply the covariance of the separately normalized $X$ and $Y$ data. Because the data have been normalized, the correlation is a unit free quantity and thus has more of a hope of being interpretable across settings.

## Some Facts of Correlation

1. The order of the arguments is irrelevant $Cor(X; Y ) = Cor(Y;X)$ 
2. It has to be between -1 and 1, $1\leq Cor(X; Y )\leq1$. 
3. The correlation is exactly -1 or 1 only when the observations fall perfectly on a negatively or positively sloped, line, respectively. 
4. $Cor(X; Y )$ measures the strength of the linear relationship between the two variables, with stronger relationships as $Cor(X; Y )$ heads towards -1 or 1.
5. $Cor(X; Y )$ = 0 implies no linear relationship.





